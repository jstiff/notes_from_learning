# Parsing.

Parsing is taking something that has no structure, and giving it structure.

HTML (and CSS) which is parsed into a DOM

Most programming is parsed into an AST before being interpreted or compiled into native binary (machine code), or byte compiled (into operations for a virtual machine processing engine such as LLVM or the JVM or the VM built into Python, etc)

#### Context-Free Grammers

- these grammers are NOT accepted by FSA like we see in Regular languages.
- These recognize a larger class of languages...not modeled as a 'machine'.
  have practical applications in languages and compilier design.
- Context free grammers are used in the building of Parsers...

---

- To generate a string in the language, one begins with a string consisting of only a single start symbol, and then successively applies the rules (any number of times, in any order) to rewrite this string. This stops when we obtain a string containing only terminals.

- Terminal symbols: similar to the alphabet like in FA. These are the characters that make up the actual content of the final sentence.

- Variable symbols (Non Terminal): Can be replaced with a string of variables and terminals.
  - Non terminals are placeholders for the terminals. We can use non terminals to generate different patterns of terminal symbols.
  - These can be replaced with the help of 'Production rules'...which dictate how variables get replaced.
  - Start Variable: starting point of the computation.

#### How computation happens...

1. Write down the 'start vaiable' as the current string. a start symbol is a special non terminal that represents the initial string that will be generated by the grammar.
2. Pick a variable in the current string and replace it with one of its production rules.
3. Continue step 2 until no more variables are remain.

```
Example...

             - Terminals: {0,1}
             - Variables: {S}
             - Production Rules:                ///Production Rules dicate how vaiables get replaced.
                S --> 0S1
                S --> Epsilon

```

'Production rules' are computer science's version of 'rewrite rules' found in mathmatics.
...recursive symbol substitution that will generate new symbol sequences.

The scanner will process a file of source code and return all accepted strings as tokens that are identified by their types or their 'syntactic category'....'identifiers', 'assignment operator', 'integers', etc... It will return one 'word' at a time.

- it returns the actual 'lexeme'

- for a language which is just a set of strings ...a grammer is set of rules that when fallowed can **generate** any string in that language.

- A Grammer is comprised of...

  - G = (V, T, P, S)

  V: set of Variables (non-terminal)
  T: set of Terminals
  P: set of productions (recursive rules)
  S: start variable

  So with the 'S --> aSb | epsilon' example.
  There is one variable which is the 'S'....two terminals which are 'a' / 'b' ....the start variable is 'S' and two production rules.

  - It's convention to denote variables by Uppercase and terminals with lowercase.

  ```
  S → aSb
  S → ε

  With 'S --> aSb | epsilon'... production rules. We can derive the string aaabbb by generating a 'Derivation tree'. Parsing is 'automated' derivation based on algorithms....

  We can 'substitute'  S for aSb as many times as we want and then can end it by calling Epsilon.


                                      S
                                    / | \
                                   a  S  b
                                      |
                                    / | \
                                   a  S  b
                                      |
                                    / | \
                                   a  S  b
                                      |
                                   Epsilon

    This derivation tree is a precursor to the AST that will be generated by a parser. We need something that is 'generative' in nature to proceed in the compilation process.
  ```

  Parser is based off of algorithms that are designed to generate these trees based off of production rules and a set of strings. If it cannot derive a tree...this is what is known as a 'syntax error'.

  - The grammer for an 'even length Palidron'.
    - S --> aSa | bSb | Epsilon

```

                 Expression
                 _____________
                |            |

X12       =     abc    +   dcf

 |                |         |
identifier     idntifier  Identifier

Assign --> identifier = Expression                         Uppercase are non-terminal vaiables.
Expression --> identifier Op identifier                     lowercase 'identifier' means it's a terminal value
Op --> + | - | * | /


                            Assign
                            /  \   \
                         ident  =   Expression
                         (X12)       /   |   \
                                  ident  +   ident
                                (abc)          (dcf)

                          X12   =  abc   +   dcf


```

- Using a grammer in a 'forward' direction means to **generate a sequence**. We can set up a machine which is a generalization of a NFA that can take in a string and accept/reject it. Not only do you need an answer to the accept/reject question, but also need to find a 'parse tree' for a given string.

- **Parsing problem** ...is the problem of going from a string to a parse tree. Not just in Context free grammars, but of other grammars as well.

- Having a single variable on the left side of the production rules signifies that it is 'context free'...(???)

S --> aSb

'context sensative' grammars have more than one variable on the left side and knowing whether or not you can use it will depend on the 'context'.....(???)

#### Chomsky

Chomsky created a 'containment hierarchy' of classes of formal grammars.

A formal grammar of this type consists of a finite set of production rules (left-hand side → right-hand side), where each side consists of a finite sequence of the following symbols:

- a finite set of nonterminal symbols (indicating that some production rule can yet be applied)
- a finite set of terminal symbols (indicating that no production rule can be applied)
- a start symbol (a distinguished nonterminal symbol)

S → AB
S → ε (where ε is the empty string)
A → aS
B → b

```
In formal language theory, a context-free grammar, G, is said to be in Chomsky normal form (first described by Noam Chomsky)[1] if all of its production rules are of the form:

A → BC, or
A → a, or
S → ε,
where A, B, and C are nonterminal symbols, the letter a is a terminal symbol (a symbol that represents a constant value), S is the start symbol, and ε denotes the empty string.
```

## Machines that can recognize Context Free Languages.

#### Pushdown Automota

We may need unbounded memory to recognize context-free languages.

The finite automaton now can base its transition on both the current symbol being read and values stored in memory.

Each transition

- is based on the current input symbol and the top of the stack,
- optionally pops the top of the stack, and
- optionally pushes new symbols onto the stack.

The language of a PDA is the set of strings that the PDA accepts. Which means our languages can grow in complexity and expressiveness in parallel with the level of computational power grows.

PDA generalize NFA's...except that it has a 'stack'. Meaning it can write on to a memory device. NFA only has memory in as much as it can be encoded into the states.

Pushdown automata are used in theories about what can be computed by machines. They are more capable than finite-state machines but less capable than Turing machines.

- There seems to be a direct relationship between the expressivness of a language/grammar and how much computational power is available. When we do not have access to secondary memory devices we are limited to DFA and NFA machine models.

### Palindrome

Make a PDA that recognizes { 0^n 1^n | n >= 0 }....any number of 0's fallowed by the same number of 1's.

We can utilize a 'stack' in a Push Down Automa by consuming the 0's from a tape and immedietley pushing them on to a stack. As soon as the NFA reads the first 1 it then triggers a 'pop()' function that pops a 0 off of the stack. If the number of 1's (which is equivalent to a pop() of a 0 ) are the same then the **stack will be empty**. We then know that the string consumed from the tape was indeed a 'Palindrome'.

In PDA...when you read from the stack you are simultaneously poping it off the stack and it is gone forever. You can write it back if you want.

-      a,b --> b         Where 'a' is read from the tape...'b' is then read/pop off the stack and 'b' is put on the stack.
- A transition in a PDA is of the form **a,b -- c** where the first character 'a' is read from a tape...the next character 'b' is read and popped off the stack and then 'c' is what is written to the stack.

## Top Down Parsing

- GCC and LLVM-Clang are using handwritten recursive descent parsers.
- Bison-Flex based, use bottom up parsing

- LL(1) parsing:
  L --> scanning input from left to right.
  L --> leftmost derivation.
  1 --> one character look ahead.

- predictive parser: type that does not require 'backtracking'. With one lookahead character which is the best solution.

remeber that parsing is about matching an input string that has been validated by the scanner with a grammer.

- Top down Parsers **cannot handle** 'left recursion'....**S --> S alpha**...where S = Non-terminal and alpha is a string of grammer symbols. This will never terminate and keep genereating Non-terminal variables. You will have to eliminate 'left recursion' through a trick...converting from left to right recursion....(look it up???).

- having a 'grammer' with a set of production rules also needs an 'order of operations' ...left , right derivations...there can be many combinations or ways to derive a string from a set of production rules. This means a grammer is **'ambiguous'**.

## Parse tree.

[Click...paper on parse-trees](<https://eng.libretexts.org/Bookshelves/Computer_Science/Book%3A_Foundations_of_Computation_(Critchlow_and_Eck)/04%3A_Grammars/4.03%3A_Parsing_and_Parse_Trees>)

- A parse tree for a grammar G is a tree where

  - the root is the start symbol for G
  - the interior nodes are the nonterminals of G
  - the leaf nodes are the terminal symbols of G.
  - the children of a node T (from left to right) correspond to the symbols on the right hand side of some production for T in G.

  Every terminal string generated by a grammar has a corresponding parse tree; every valid parse tree represents a string generated by the grammar (called the yield of the parse tree).
